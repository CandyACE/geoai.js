model:
  base_model: "distilbert-base-uncased"
  num_classes: 3
  max_sequence_length: 128

training:
  learning_rate: 2.0e-5
  batch_size: 16
  epochs: 20
  warmup_steps: 500
  weight_decay: 0.01
  gradient_accumulation_steps: 1
  seed: 42
  output_dir: "./output"
  logging_steps: 100
  eval_steps: 500
  save_steps: 1000

data:
  train_split: 0.8
  val_split: 0.1
  test_split: 0.1
  data_dir: "./data/processed"
